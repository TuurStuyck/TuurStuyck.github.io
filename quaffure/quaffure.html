
<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Quaffure: Real-Time Quasi-Static Neural Hair Simulation">

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Neural Hair Simulation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Quaffure: Real-Time Quasi-Static Neural Hair Simulation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Quaffure: Real-Time Quasi-Static Neural Hair Simulation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tuurstuyck.github.io/" target="_blank">Tuur Stuyck</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://tuurstuyck.github.io/" target="_blank">Gene Wei-Chin Lin</a><sup>*</sup>,</span>
              <span class="author-block">
                <a href="https://elrnv.com/" target="_blank">Egor Larionov</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/hsiaoyu" target="_blank">Hsiao-yu Chen</a>,</span>
              <span class="author-block">
                <a href="https://aljazbozic.github.io/" target="_blank">Aljaž Božič</a>,</span>
              <span class="author-block">
                <a href="https://nsarafianos.github.io/" target="_blank">Nikolaos Sarafianos</a>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/doug-roble-752a081" target="_blank">Doug Roble</a></span>
            </div>

            <div class="is-size-4 publication-authors">
              <span class="author-block">Meta Reality Labs</span>
            </div>
            <div class="is-size-10 publication-authors">
              <span class="author-block"><sup>*</sup>Equal Contributions</span>
            </div>

            <font size="+2"><b>CVPR 2025</b></font>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2412.10061" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2412.10061" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=_kaSzSsfJuQ"
                   class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                  <span>Video</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <!-- Paper abstract -->
  <section class="section hero">
    <div class="container is-max-desktop">


      <!-- Overview. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <hr class="divider" />
          <div class="overview-image">
            <img src="./static/images/teaser.jpg" type="image/jpg">
          </div>
          <div class="content has-text-justified">
            <p>
              We present Quaffure, a real-time quasi-static neural hair simulator, which produces naturally draped hair in only a few milliseconds on commodity hardware, taking the hairstyle, body shape and pose into account. Our method scales to predicting the drape of 1000 hair grooms in just 0.3 seconds. Quaffure is trained using a physics-based self-supervised loss, eliminating the need for simulated training data that is costly and cumbersome to obtain. We show that our method works for a wide variety of body shapes and poses with a range of hairstyles varying from straight to curly, short to long.
            </p>
          </div>
        </div>
      </div>
      <!--/ Overview. -->


      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
            Realistic hair motion is crucial for high-quality avatars, but it is often limited by the computational resources available for real-time applications. To address this challenge, we propose a novel neural approach to predict physically plausible hair deformations that generalizes to various body poses, shapes, and hairstyles. 
            Our model is trained using a self-supervised loss, eliminating the need for expensive data generation and storage. We demonstrate our method's effectiveness through numerous results across a wide range of pose and shape variations, showcasing its robust generalization capabilities and temporally smooth results. 
            Our approach is highly suitable for real-time applications with an inference time of only a few milliseconds on consumer hardware and its ability to scale to predicting the drape of 1000 grooms in 0.3 seconds. 
            </p>
          </div>
        </div>
      </div>
      <!-- End paper abstract -->

      <!-- Overview. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <hr class="divider" />
          <h2 class="title is-3">Method Overview</h2>
          <div class="overview-image">
            <img src="./static/images/overview.jpg" type="image/jpg">
          </div>
          <div class="content has-text-justified">
            <p>
            Our method takes a code as input, consisting of a latent code for the rest hair shape, body shape parameters, and full skeleton pose. 
            The output is naturally draped hair produced as the sum of posed hair given the body pose and shape parameters, combined with learned corrections which are produced by the groom deformation decoder. 
            We train our method in two stages: i) an autoencoder is trained on all hairstyles to obtain a groom latent code, and ii) the groom deformation decoder is trained in a physics-based self-supervised fashion. The hair strands are encoded in a 2D texture representation (left) where strands are encoded in the pixel in which the root particle is located. The figure shows how the 3D scalp geometry (bottom) is mapped to a high dimensional 2D texture map (top).
            </p>
          </div>
        </div>
      </div>
      <!--/ Overview. -->

            <!-- Overview. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <hr class="divider" />
          <h2 class="title is-3">Results</h2>
          <div class="overview-image">
            <img src="./static/images/gallery.jpg" type="image/jpg">
          </div>
          <div class="content has-text-justified">
            <p>
            We showcase the versatility of our method in modeling the quasi-static behavior of a plethora of hairstyles and lengths for different body shapes and poses where all results are obtained using the same settings without any manual parameter tuning.
            Our results cover short/medium/long hair, with various levels of curliness, including straight, wavy, curly, and kinky.
            </p>
          </div>
        </div>
      </div>
      <!--/ Overview. -->

    </div>
  </section>

  <!-- Gallery video-->
  <section class="hero gallery">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <h2 class="title is-3">Video</h2>
        </div>

        <iframe width="560" height="315"
        src="https://www.youtube.com/embed/_kaSzSsfJuQ"
        frameborder="0"
        allowfullscreen>
        </iframe>
    </div>
  </section>

  
  
  <!--BibTex citation -->
  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{stuyck2024quaffure,
  title={Quaffure: Real-Time Quasi-Static Neural Hair Simulation},
  author={Stuyck, Tuur and Lin, Gene Wei-Chin and Larionov, Egor and Chen, Hsiao-yu and Bozic, Aljaz and Sarafianos, Nikolaos and Roble, Doug},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
  </code></pre>
  </div>
</section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the source code of this website, we just ask that you link back to this page in the
              footer. <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>
